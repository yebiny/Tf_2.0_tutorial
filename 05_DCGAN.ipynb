{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05-DCGAN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yebiny/GANexample/blob/master/05_DCGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppDNOL5w5Gm1",
        "colab_type": "text"
      },
      "source": [
        "# DCGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUQDNPta5H2s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "f2ade816-863a-42cf-b6f8-7860c08e83e9"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        " \n",
        "# mnist data 불러옴\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "mnist = input_data.read_data_sets(\"./mnist/data/\", one_hot=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting ./mnist/data/train-images-idx3-ubyte.gz\n",
            "Extracting ./mnist/data/train-labels-idx1-ubyte.gz\n",
            "Extracting ./mnist/data/t10k-images-idx3-ubyte.gz\n",
            "Extracting ./mnist/data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMN33sac7L-2",
        "colab_type": "text"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzbJjE0L5JUM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# epoch, batch size, noise 로 쓸 상수 변수 선언\n",
        "total_epoch = 100\n",
        "batch_size = 100\n",
        "n_noise = 100\n",
        " \n",
        "# 전역 변수로 쓸 변수 0으로 초기화\n",
        "D_global_step = tf.Variable(0, trainable=False, name='D_global_step')\n",
        "G_global_step = tf.Variable(0, trainable=False, name='G_global_step')\n",
        " \n",
        "# 이미지를 담는 변수 X와 노이즈 변수 Z\n",
        "X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
        "Z = tf.placeholder(tf.float32, [None, n_noise])\n",
        "\n",
        "is_training = tf.placeholder(tf.bool)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Me4pjPfo5TrJ",
        "colab_type": "text"
      },
      "source": [
        "## Leak ReLU\n",
        "* ReLU의 경우 음수영역에서 0을 출력하지만 이 함수는 음수영역에서 약간의 기울기를 가집니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXHZ0kCQ5YV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def leaky_relu(x, leak=0.2):\n",
        "    return tf.maximum(x, x * leak)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P-wn1AVc5hUF",
        "colab_type": "text"
      },
      "source": [
        "## Generator model\n",
        "* DCGAN의 가장 큰 특징은 convolution lyaer와 batch normalization이고 마지막 output은 thnh가 사용됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRyOpudM5jis",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator(noise):\n",
        "    with tf.variable_scope('generator'):\n",
        "        output = tf.layers.dense(noise, 128*7*7)\n",
        "        output = tf.reshape(output, [-1, 7, 7, 128])\n",
        "        output = tf.nn.relu(tf.layers.batch_normalization(output, training=is_training))\n",
        "        output = tf.layers.conv2d_transpose(output, 64, [5, 5], strides=(2, 2), padding='SAME')\n",
        "        output = tf.nn.relu(tf.layers.batch_normalization(output, training=is_training))\n",
        "        output = tf.layers.conv2d_transpose(output, 32, [5, 5], strides=(2, 2), padding='SAME')\n",
        "        output = tf.nn.relu(tf.layers.batch_normalization(output, training=is_training))\n",
        "        output = tf.layers.conv2d_transpose(output, 1, [5, 5], strides=(1, 1), padding='SAME')\n",
        "        output = tf.tanh(output)\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5EecswS50w4",
        "colab_type": "text"
      },
      "source": [
        "## Discriminator model\n",
        "* leaky_relu와 convolution lyaer를 사용합니다.\n",
        "* 마지막에서 flatten 해줍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6EkTfBv6RzM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discriminator(inputs, reuse=None):\n",
        "    with tf.variable_scope('discriminator') as scope:\n",
        "        \n",
        "        # scope.reuse_variables()이후에는 같은 scope내에서 같은 name을 갖는 변수를 새로 만들지 않고 재사용 한다.\n",
        "        if reuse:\n",
        "            scope.reuse_variables()\n",
        "            \n",
        "        output = tf.layers.conv2d(inputs, 32, [5, 5], strides=(\n",
        "            2, 2), padding='SAME')\n",
        "        output = leaky_relu(output)\n",
        "        output = tf.layers.conv2d(output, 64, [5, 5], strides=(2, 2), padding='SAME')\n",
        "        output = leaky_relu(tf.layers.batch_normalization(output, training=is_training))\n",
        "        output = tf.layers.conv2d(output, 128, [5, 5], strides=(2, 2), padding='SAME')\n",
        "        output = leaky_relu(tf.layers.batch_normalization(output, training=is_training))\n",
        "        flat = tf.contrib.layers.flatten(output)\n",
        "        output = tf.layers.dense(flat, 1, activation=None)\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDy4u-dh7BAl",
        "colab_type": "text"
      },
      "source": [
        "## Noise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fObMx-GZ7COc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_noise(batch_size, n_noise):\n",
        "    return np.random.uniform(-1.0, 1.0, size=[batch_size, n_noise])\n",
        "  \n",
        "# interpolation 되는 과정을 보기위해 노이즈를 두개 생성하는 부분\n",
        "def get_moving_noise(batch_size, n_noise):\n",
        "    assert batch_size > 0\n",
        " \n",
        "    # 노이즈 두개 생성\n",
        "    noise_list = []\n",
        "    base_noise = np.random.uniform(-1.0, 1.0, size=[n_noise])\n",
        "    end_noise = np.random.uniform(-1.0, 1.0, size=[n_noise])\n",
        " \n",
        "    # 임의의 step 설정\n",
        "    step = (end_noise - base_noise) / batch_size\n",
        "    noise = np.copy(base_noise)\n",
        "    for _ in range(batch_size - 1):\n",
        "        noise_list.append(noise)\n",
        "        noise = noise + step\n",
        "    noise_list.append(end_noise)   # 변해가는 모습을 볼 수 있게 base 부터 end 까지 노이즈 리스트 만듦\n",
        "    \n",
        "    return noise_list  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIty1KlM7ZHt",
        "colab_type": "text"
      },
      "source": [
        "## Loss: Cross Entropy\n",
        "* Cross entropy는주로 범주형 데이터에 사용되는 손실함ㅅ 입니다. \n",
        "* Cross entropy는 두 분포 사이의 유사성을 측정하는 척도입니다. 딥러닝에서의 분류 모델은 각 클래스의 확률값을 계산하므로 실제 클래스(y_true)와 모델이 예측한 클래스(y_pred)를 비교할 수 있으며 두 분포가 가까울수록 이 값은 더 작아집니다. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fB2I67zm71ev",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "outputId": "eb97611c-d01b-4a7b-f52d-7989230f7e30"
      },
      "source": [
        "G = generator(Z)\n",
        "D_real = discriminator(X)\n",
        "D_gene = discriminator(G, True)\n",
        "\n",
        "# loss 함수 cross entropy 사용\n",
        "loss_D_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real, labels=tf.ones_like(D_real)))\n",
        "loss_D_gene = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_gene, labels=tf.zeros_like(D_gene)))\n",
        "loss_D = loss_D_real + loss_D_gene\n",
        "loss_G = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_gene, labels=tf.ones_like(D_gene)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0905 05:40:48.520984 139814144010112 deprecation.py:323] From <ipython-input-8-b822b833a4b3>:3: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "W0905 05:40:48.533602 139814144010112 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0905 05:40:48.829274 139814144010112 deprecation.py:323] From <ipython-input-8-b822b833a4b3>:5: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
            "W0905 05:40:48.943234 139814144010112 deprecation.py:323] From <ipython-input-8-b822b833a4b3>:6: conv2d_transpose (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2DTranspose` instead.\n",
            "W0905 05:40:49.823045 139814144010112 deprecation.py:323] From <ipython-input-9-474068e3a6f8>:9: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.keras.layers.Conv2D` instead.\n",
            "W0905 05:40:50.235160 139814144010112 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/layers/python/layers/layers.py:1634: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.flatten instead.\n",
            "W0905 05:40:50.627697 139814144010112 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCFmqBL173Tr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tensorflow의 namefield를 이용해 변수를 재활용하기 위해 해당 collection의 variable들을 불러온다. \n",
        "# tf.get_collection(key)가 실행되면, key의 collection에 속하는 variable들의 리스트가 리턴된다.\n",
        "# 출처: https://eyeofneedle.tistory.com/24 [Technology worth spreading]\n",
        "\n",
        "vars_D = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope='discriminator')\n",
        "vars_G = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope='generator')\n",
        " \n",
        "# training 시점에서 변수 update_ops에 업데이트가 저장됨. 그래서 update_ops를 학습할 때 넣어준다.\n",
        "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJ6B4_En8CIV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5cd2b811-8e88-45d4-b14c-9d50a75d4b77"
      },
      "source": [
        "# loss를 adamoptimizer를 이용해서 minimize하게 학습\n",
        "with tf.control_dependencies(update_ops):\n",
        "    train_D = tf.train.AdamOptimizer().minimize(loss_D,var_list=vars_D, global_step=D_global_step)\n",
        "    train_G = tf.train.AdamOptimizer().minimize(loss_G,var_list=vars_G, global_step=G_global_step)\n",
        "    \n",
        "# TensorBoard 사용하게 위해 loss값들을 기록\n",
        "tf.summary.scalar('loss_D', loss_D)\n",
        "tf.summary.scalar('loss_G', loss_G)    "
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'loss_G:0' shape=() dtype=string>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2PcfFAf8KcW",
        "colab_type": "text"
      },
      "source": [
        "## Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fVuUUDG8Mb8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "582127ec-8000-4abc-e3f0-46a5b0559dff"
      },
      "source": [
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        " \n",
        "    merged = tf.summary.merge_all()\n",
        "    writer = tf.summary.FileWriter('./logs', sess.graph)  # 로그로 저장\n",
        " \n",
        "    total_batch = int(mnist.train.num_examples / batch_size)  # 전체 배치 사이즈 계산\n",
        " \n",
        "    for epoch in range(total_epoch):\n",
        "        loss_val_D, loss_val_G = 0, 0\n",
        " \n",
        "        batch_xs, batch_ys = None, None\n",
        "        noise = None\n",
        " \n",
        "        for i in range(total_batch):\n",
        "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "            batch_xs = batch_xs.reshape(-1, 28, 28, 1)\n",
        "            noise = get_noise(batch_size, n_noise)\n",
        " \n",
        "            # 학습 돌림 \n",
        "            _, loss_val_D = sess.run([train_D, loss_D],feed_dict={X: batch_xs, Z: noise, is_training: True})\n",
        "            _, loss_val_G = sess.run([train_G, loss_G],feed_dict={X: batch_xs, Z: noise, is_training: True})\n",
        " \n",
        "        summary = sess.run(merged,feed_dict={X: batch_xs, Z: noise, is_training: True})\n",
        "        writer.add_summary(summary, global_step=sess.run(G_global_step)) # log로 저장\n",
        " \n",
        "        # epoch과 loss 프린트\n",
        "        print('Epoch:', '%04d' % epoch,\n",
        "            'D loss: {:.4}'.format(loss_val_D),\n",
        "            'G loss: {:.4}'.format(loss_val_G))\n",
        "     \n",
        "        # 5번째 마다 그림 보여줌 및 저장\n",
        "        if epoch == 0 or (epoch + 1) % 5 == 0:\n",
        "            sample_size = 10\n",
        "            noise = get_noise(sample_size, n_noise)\n",
        "            samples = sess.run(G, feed_dict={Z: noise, is_training: False})\n",
        "            test_noise = get_moving_noise(sample_size, n_noise)  # 두개의 노이즈 만들고 그 사이에 step을 더해서 noise list 가져옴\n",
        "            test_samples = sess.run(G, feed_dict={Z: test_noise, is_training: False})\n",
        " \n",
        "            fig, ax = plt.subplots(2, sample_size, figsize=(sample_size, 2))\n",
        " \n",
        "            for i in range(sample_size):\n",
        "                ax[0][i].set_axis_off()\n",
        "                ax[1][i].set_axis_off()\n",
        "                ax[0][i].imshow(np.reshape(samples[i], (28, 28)))  # generator가 만든 그림들\n",
        "                ax[1][i].imshow(np.reshape(test_samples[i], (28, 28)))  # 두개의 노이즈를 만들어 interpolation되어가는 과정을 보여준 그림\n",
        "             \n",
        "            \n",
        "            plt.savefig('{}.png'.format(str(epoch).zfill(3)),bbox_inches='tight')\n",
        "            \n",
        "            plt.close(fig)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0000 D loss: 0.1153 G loss: 9.229\n",
            "Epoch: 0001 D loss: 0.0597 G loss: 4.931\n",
            "Epoch: 0002 D loss: 0.08796 G loss: 6.33\n",
            "Epoch: 0003 D loss: 0.04069 G loss: 5.273\n",
            "Epoch: 0004 D loss: 0.06501 G loss: 5.908\n",
            "Epoch: 0005 D loss: 0.3673 G loss: 3.824\n",
            "Epoch: 0006 D loss: 0.4463 G loss: 3.121\n",
            "Epoch: 0007 D loss: 0.3865 G loss: 3.38\n",
            "Epoch: 0008 D loss: 0.4037 G loss: 3.401\n",
            "Epoch: 0009 D loss: 0.1977 G loss: 3.38\n",
            "Epoch: 0010 D loss: 0.9338 G loss: 2.324\n",
            "Epoch: 0011 D loss: 0.6475 G loss: 2.663\n",
            "Epoch: 0012 D loss: 1.285 G loss: 2.18\n",
            "Epoch: 0013 D loss: 0.5323 G loss: 3.138\n",
            "Epoch: 0014 D loss: 0.4924 G loss: 2.454\n",
            "Epoch: 0015 D loss: 0.8241 G loss: 3.514\n",
            "Epoch: 0016 D loss: 0.5668 G loss: 2.194\n",
            "Epoch: 0017 D loss: 0.8821 G loss: 2.361\n",
            "Epoch: 0018 D loss: 0.5723 G loss: 2.578\n",
            "Epoch: 0019 D loss: 0.4671 G loss: 3.588\n",
            "Epoch: 0020 D loss: 0.2647 G loss: 3.455\n",
            "Epoch: 0021 D loss: 0.3037 G loss: 1.852\n",
            "Epoch: 0022 D loss: 0.2212 G loss: 3.732\n",
            "Epoch: 0023 D loss: 0.8343 G loss: 2.539\n",
            "Epoch: 0024 D loss: 0.4532 G loss: 2.095\n",
            "Epoch: 0025 D loss: 0.4398 G loss: 2.991\n",
            "Epoch: 0026 D loss: 0.3079 G loss: 2.988\n",
            "Epoch: 0027 D loss: 0.3148 G loss: 2.637\n",
            "Epoch: 0028 D loss: 0.5796 G loss: 2.834\n",
            "Epoch: 0029 D loss: 0.9029 G loss: 2.782\n",
            "Epoch: 0030 D loss: 0.7165 G loss: 1.692\n",
            "Epoch: 0031 D loss: 0.461 G loss: 2.807\n",
            "Epoch: 0032 D loss: 0.4527 G loss: 2.073\n",
            "Epoch: 0033 D loss: 0.9932 G loss: 2.699\n",
            "Epoch: 0034 D loss: 0.453 G loss: 2.814\n",
            "Epoch: 0035 D loss: 0.6411 G loss: 1.245\n",
            "Epoch: 0036 D loss: 0.4067 G loss: 2.642\n",
            "Epoch: 0037 D loss: 0.3421 G loss: 2.421\n",
            "Epoch: 0038 D loss: 0.4247 G loss: 2.609\n",
            "Epoch: 0039 D loss: 0.3613 G loss: 3.728\n",
            "Epoch: 0040 D loss: 0.3087 G loss: 3.295\n",
            "Epoch: 0041 D loss: 1.307 G loss: 4.895\n",
            "Epoch: 0042 D loss: 0.6082 G loss: 2.396\n",
            "Epoch: 0043 D loss: 0.4438 G loss: 3.063\n",
            "Epoch: 0044 D loss: 0.865 G loss: 1.367\n",
            "Epoch: 0045 D loss: 0.3116 G loss: 3.396\n",
            "Epoch: 0046 D loss: 1.293 G loss: 1.964\n",
            "Epoch: 0047 D loss: 0.3735 G loss: 2.864\n",
            "Epoch: 0048 D loss: 0.4135 G loss: 3.922\n",
            "Epoch: 0049 D loss: 0.3496 G loss: 3.73\n",
            "Epoch: 0050 D loss: 0.369 G loss: 1.581\n",
            "Epoch: 0051 D loss: 0.4289 G loss: 4.665\n",
            "Epoch: 0052 D loss: 0.1852 G loss: 3.733\n",
            "Epoch: 0053 D loss: 0.5799 G loss: 5.675\n",
            "Epoch: 0054 D loss: 0.3538 G loss: 2.216\n",
            "Epoch: 0055 D loss: 0.3062 G loss: 3.349\n",
            "Epoch: 0056 D loss: 0.6019 G loss: 2.692\n",
            "Epoch: 0057 D loss: 0.3376 G loss: 3.361\n",
            "Epoch: 0058 D loss: 0.1815 G loss: 3.311\n",
            "Epoch: 0059 D loss: 0.218 G loss: 4.92\n",
            "Epoch: 0060 D loss: 1.226 G loss: 0.9504\n",
            "Epoch: 0061 D loss: 0.1646 G loss: 2.725\n",
            "Epoch: 0062 D loss: 0.8051 G loss: 3.238\n",
            "Epoch: 0063 D loss: 0.1308 G loss: 4.868\n",
            "Epoch: 0064 D loss: 0.8032 G loss: 1.61\n",
            "Epoch: 0065 D loss: 0.2711 G loss: 5.22\n",
            "Epoch: 0066 D loss: 2.345 G loss: 0.7794\n",
            "Epoch: 0067 D loss: 0.2732 G loss: 2.588\n",
            "Epoch: 0068 D loss: 0.1014 G loss: 6.486\n",
            "Epoch: 0069 D loss: 0.173 G loss: 4.613\n",
            "Epoch: 0070 D loss: 0.7231 G loss: 5.308\n",
            "Epoch: 0071 D loss: 0.2395 G loss: 4.303\n",
            "Epoch: 0072 D loss: 0.09617 G loss: 3.49\n",
            "Epoch: 0073 D loss: 0.3831 G loss: 2.321\n",
            "Epoch: 0074 D loss: 0.5678 G loss: 2.79\n",
            "Epoch: 0075 D loss: 0.1762 G loss: 4.675\n",
            "Epoch: 0076 D loss: 0.13 G loss: 4.437\n",
            "Epoch: 0077 D loss: 0.2343 G loss: 5.35\n",
            "Epoch: 0078 D loss: 0.4731 G loss: 2.59\n",
            "Epoch: 0079 D loss: 0.5203 G loss: 4.067\n",
            "Epoch: 0080 D loss: 0.1605 G loss: 3.912\n",
            "Epoch: 0081 D loss: 0.2947 G loss: 3.796\n",
            "Epoch: 0082 D loss: 0.1775 G loss: 2.503\n",
            "Epoch: 0083 D loss: 0.1339 G loss: 3.916\n",
            "Epoch: 0084 D loss: 0.08481 G loss: 5.104\n",
            "Epoch: 0085 D loss: 0.1653 G loss: 4.389\n",
            "Epoch: 0086 D loss: 0.3259 G loss: 2.275\n",
            "Epoch: 0087 D loss: 0.3271 G loss: 3.387\n",
            "Epoch: 0088 D loss: 0.08598 G loss: 3.886\n",
            "Epoch: 0089 D loss: 2.076 G loss: 0.9488\n",
            "Epoch: 0090 D loss: 0.156 G loss: 3.999\n",
            "Epoch: 0091 D loss: 0.2223 G loss: 3.782\n",
            "Epoch: 0092 D loss: 0.2172 G loss: 2.682\n",
            "Epoch: 0093 D loss: 0.2364 G loss: 4.526\n",
            "Epoch: 0094 D loss: 0.3732 G loss: 3.746\n",
            "Epoch: 0095 D loss: 0.1095 G loss: 4.39\n",
            "Epoch: 0096 D loss: 0.4513 G loss: 2.838\n",
            "Epoch: 0097 D loss: 0.1932 G loss: 4.426\n",
            "Epoch: 0098 D loss: 0.1131 G loss: 3.164\n",
            "Epoch: 0099 D loss: 0.2131 G loss: 3.11\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}